{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJcR73F8hKdr"
      },
      "source": [
        "# Upload multiple CSV files from a Google Cloud Bucket to a Big Query dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZx1hcbzhmTj"
      },
      "source": [
        "Package installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_uW1GJ2iNjS"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "from google.cloud import bigquery\n",
        "import pandas as pd\n",
        "\n",
        "# Authenticate with Google Cloud\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Set up BigQuery client\n",
        "client = bigquery.Client(project='ty-mini-project-41')\n",
        "\n",
        "# Function to create a new table in BigQuery\n",
        "def create_table(dataset_id, table_id, schema):\n",
        "    dataset_ref = client.dataset(dataset_id)\n",
        "    table_ref = dataset_ref.table(table_id)\n",
        "\n",
        "\n",
        "    table = bigquery.Table(table_ref, schema=schema)\n",
        "    table = client.create_table(table)\n",
        "    print(f'Table {table.table_id} created successfully.')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEx64Vy95gH8"
      },
      "outputs": [],
      "source": [
        "# Function to read a csv file from a Google cloud Bucket and push its content to a table in Big Query\n",
        "def load_csv_to_table(dataset_id, table_id, csv_path):\n",
        "    dataset_ref = client.dataset(dataset_id)\n",
        "    table_ref = dataset_ref.table(table_id)\n",
        "\n",
        "    job_config = bigquery.LoadJobConfig()\n",
        "    job_config.skip_leading_rows = 1  # Skip the header row\n",
        "\n",
        "    with open(csv_path, 'rb') as source_file:\n",
        "        job = client.load_table_from_file(source_file, table_ref, job_config=job_config)\n",
        "\n",
        "    job.result()  # Wait for the job to complete\n",
        "\n",
        "    print(f'CSV file {csv_path} loaded into table {table_id} successfully.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjK0mQhHkZUC"
      },
      "source": [
        "The following functions were included to standarize date formats within the files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DjfmHb_-K9B"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "#find object columns\n",
        "\n",
        "def change_time(x):\n",
        "    format = '%m/%d/%Y %I:%M:%S %p'\n",
        "    my_date = datetime.strptime(x, format)\n",
        "    return my_date\n",
        "\n",
        "\n",
        "def change_types(x):\n",
        "  if x.count('int')>0:\n",
        "    return \"NUMERIC\"\n",
        "  elif x.count('date')>0:\n",
        "    return 'DATETIME'\n",
        "  elif x.count('float')>0:\n",
        "    return 'BIGNUMERIC'\n",
        "  else:\n",
        "    return 'STRING'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrYET-pW0sck"
      },
      "source": [
        "The process need to receive the name of each file that you would like to add, so i included this function to list all files in a google cloud bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ng2L0jjprp0t",
        "outputId": "099da80e-acb9-4a63-db78-2b83b4ad0750"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Blob: final_preprocessed_files_when2heat, AT_preprocessed_data.csv, 1714045831210248>\n",
            "<Blob: final_preprocessed_files_when2heat, BE_preprocessed_data.csv, 1714045923492032>\n",
            "<Blob: final_preprocessed_files_when2heat, BG_preprocessed_data.csv, 1714045943516977>\n",
            "<Blob: final_preprocessed_files_when2heat, CH_preprocessed_data.csv, 1714045958313634>\n",
            "<Blob: final_preprocessed_files_when2heat, CZ_preprocessed_data.csv, 1714045887237287>\n",
            "<Blob: final_preprocessed_files_when2heat, DE_preprocessed_data.csv, 1714045965651638>\n",
            "<Blob: final_preprocessed_files_when2heat, DK_preprocessed_data.csv, 1714045942192461>\n",
            "<Blob: final_preprocessed_files_when2heat, EE_preprocessed_data.csv, 1714045814511687>\n",
            "<Blob: final_preprocessed_files_when2heat, ES_preprocessed_data.csv, 1714045922241203>\n",
            "<Blob: final_preprocessed_files_when2heat, FI_preprocessed_data.csv, 1714045907839405>\n",
            "<Blob: final_preprocessed_files_when2heat, FR_preprocessed_data.csv, 1714045936814214>\n",
            "<Blob: final_preprocessed_files_when2heat, GB_preprocessed_data.csv, 1714045960419735>\n",
            "<Blob: final_preprocessed_files_when2heat, GR_preprocessed_data.csv, 1714045848409507>\n",
            "<Blob: final_preprocessed_files_when2heat, HR_preprocessed_data.csv, 1714045970474029>\n",
            "<Blob: final_preprocessed_files_when2heat, HU_preprocessed_data.csv, 1714045815110774>\n",
            "<Blob: final_preprocessed_files_when2heat, IE_preprocessed_data.csv, 1714045868359181>\n",
            "<Blob: final_preprocessed_files_when2heat, IT_preprocessed_data.csv, 1714045815008473>\n",
            "<Blob: final_preprocessed_files_when2heat, LT_preprocessed_data.csv, 1714045887006732>\n",
            "<Blob: final_preprocessed_files_when2heat, LU_preprocessed_data.csv, 1714045830909519>\n",
            "<Blob: final_preprocessed_files_when2heat, LV_preprocessed_data.csv, 1714045907592500>\n",
            "<Blob: final_preprocessed_files_when2heat, NL_preprocessed_data.csv, 1714045847970763>\n",
            "<Blob: final_preprocessed_files_when2heat, NO_preprocessed_data.csv, 1714045917598942>\n",
            "<Blob: final_preprocessed_files_when2heat, PL_preprocessed_data.csv, 1714045868675871>\n",
            "<Blob: final_preprocessed_files_when2heat, PT_preprocessed_data.csv, 1714045886842944>\n",
            "<Blob: final_preprocessed_files_when2heat, RO_preprocessed_data.csv, 1714045907443206>\n",
            "<Blob: final_preprocessed_files_when2heat, SE_preprocessed_data.csv, 1714045830410341>\n",
            "<Blob: final_preprocessed_files_when2heat, SI_preprocessed_data.csv, 1714045847969691>\n",
            "<Blob: final_preprocessed_files_when2heat, SK_preprocessed_data.csv, 1714045868308558>\n"
          ]
        }
      ],
      "source": [
        "from google.cloud import storage\n",
        "\n",
        "def list_files_in_bucket(bucket_name):\n",
        "    # Initialize a client\n",
        "    client = storage.Client()\n",
        "\n",
        "    for blob in client.list_blobs(bucket_name):\n",
        "      print(str(blob))\n",
        "\n",
        "\n",
        "list_files_in_bucket('final_preprocessed_files_when2heat')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_h0-LwmDuIf",
        "outputId": "f265a042-244a-4260-f8f0-7bef35a846a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Table AT_preprocessed_data created successfully.\n",
            "CSV file AT_preprocessed_data loaded into table AT_preprocessed_data successfully.\n",
            "Table BE_preprocessed_data created successfully.\n",
            "CSV file BE_preprocessed_data loaded into table BE_preprocessed_data successfully.\n",
            "Table BG_preprocessed_data created successfully.\n",
            "CSV file BG_preprocessed_data loaded into table BG_preprocessed_data successfully.\n",
            "Table CH_preprocessed_data created successfully.\n",
            "CSV file CH_preprocessed_data loaded into table CH_preprocessed_data successfully.\n",
            "Table CZ_preprocessed_data created successfully.\n",
            "CSV file CZ_preprocessed_data loaded into table CZ_preprocessed_data successfully.\n",
            "Table DE_preprocessed_data created successfully.\n",
            "CSV file DE_preprocessed_data loaded into table DE_preprocessed_data successfully.\n",
            "Table DK_preprocessed_data created successfully.\n",
            "CSV file DK_preprocessed_data loaded into table DK_preprocessed_data successfully.\n",
            "Table EE_preprocessed_data created successfully.\n",
            "CSV file EE_preprocessed_data loaded into table EE_preprocessed_data successfully.\n",
            "Table ES_preprocessed_data created successfully.\n",
            "CSV file ES_preprocessed_data loaded into table ES_preprocessed_data successfully.\n",
            "Table FI_preprocessed_data created successfully.\n",
            "CSV file FI_preprocessed_data loaded into table FI_preprocessed_data successfully.\n",
            "Table FR_preprocessed_data created successfully.\n",
            "CSV file FR_preprocessed_data loaded into table FR_preprocessed_data successfully.\n",
            "Table GB_preprocessed_data created successfully.\n",
            "CSV file GB_preprocessed_data loaded into table GB_preprocessed_data successfully.\n",
            "Table GR_preprocessed_data created successfully.\n",
            "CSV file GR_preprocessed_data loaded into table GR_preprocessed_data successfully.\n",
            "Table HR_preprocessed_data created successfully.\n",
            "CSV file HR_preprocessed_data loaded into table HR_preprocessed_data successfully.\n",
            "Table HU_preprocessed_data created successfully.\n",
            "CSV file HU_preprocessed_data loaded into table HU_preprocessed_data successfully.\n",
            "Table IE_preprocessed_data created successfully.\n",
            "CSV file IE_preprocessed_data loaded into table IE_preprocessed_data successfully.\n",
            "Table IT_preprocessed_data created successfully.\n",
            "CSV file IT_preprocessed_data loaded into table IT_preprocessed_data successfully.\n",
            "Table LT_preprocessed_data created successfully.\n",
            "CSV file LT_preprocessed_data loaded into table LT_preprocessed_data successfully.\n",
            "Table LU_preprocessed_data created successfully.\n",
            "CSV file LU_preprocessed_data loaded into table LU_preprocessed_data successfully.\n",
            "Table LV_preprocessed_data created successfully.\n",
            "CSV file LV_preprocessed_data loaded into table LV_preprocessed_data successfully.\n",
            "Table NL_preprocessed_data created successfully.\n",
            "CSV file NL_preprocessed_data loaded into table NL_preprocessed_data successfully.\n",
            "Table NO_preprocessed_data created successfully.\n",
            "CSV file NO_preprocessed_data loaded into table NO_preprocessed_data successfully.\n",
            "Table PL_preprocessed_data created successfully.\n",
            "CSV file PL_preprocessed_data loaded into table PL_preprocessed_data successfully.\n",
            "Table PT_preprocessed_data created successfully.\n",
            "CSV file PT_preprocessed_data loaded into table PT_preprocessed_data successfully.\n",
            "Table RO_preprocessed_data created successfully.\n",
            "CSV file RO_preprocessed_data loaded into table RO_preprocessed_data successfully.\n",
            "Table SE_preprocessed_data created successfully.\n",
            "CSV file SE_preprocessed_data loaded into table SE_preprocessed_data successfully.\n",
            "Table SI_preprocessed_data created successfully.\n",
            "CSV file SI_preprocessed_data loaded into table SI_preprocessed_data successfully.\n",
            "Table SK_preprocessed_data created successfully.\n",
            "CSV file SK_preprocessed_data loaded into table SK_preprocessed_data successfully.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import auth\n",
        "from google.cloud import bigquery\n",
        "import pandas as pd\n",
        "\n",
        "# Authenticate with Google Cloud\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Set up BigQuery client\n",
        "client = bigquery.Client(project='ty-mini-project-41')\n",
        "\n",
        "# Define the dataset_id\n",
        "dataset_id = 'final_preprocessed_when2heat'  # Replace with your actual dataset ID\n",
        "\n",
        "# Function to create a new table in BigQuery\n",
        "def create_table(dataset_id, table_id, schema):\n",
        "    dataset_ref = client.dataset(dataset_id)\n",
        "    table_ref = dataset_ref.table(table_id)\n",
        "\n",
        "    table = bigquery.Table(table_ref, schema=schema)\n",
        "    table = client.create_table(table)\n",
        "    print(f'Table {table.table_id} created successfully.')\n",
        "\n",
        "# Function to read a csv file from a Google cloud Bucket and push its content to a table in Big Query\n",
        "def load_csv_to_table(dataset_id, table_id, csv_path):\n",
        "    dataset_ref = client.dataset(dataset_id)\n",
        "    table_ref = dataset_ref.table(table_id)\n",
        "\n",
        "    job_config = bigquery.LoadJobConfig()\n",
        "    job_config.skip_leading_rows = 1  # Skip the header row\n",
        "\n",
        "    with open(csv_path, 'rb') as source_file:\n",
        "        job = client.load_table_from_file(source_file, table_ref, job_config=job_config)\n",
        "\n",
        "    job.result()  # Wait for the job to complete\n",
        "\n",
        "    print(f'CSV file {csv_path} loaded into table {table_id} successfully.')\n",
        "\n",
        "# Function to fix the CSV DataFrame\n",
        "from datetime import datetime\n",
        "\n",
        "def fix_the_csv(df, country_prefix):\n",
        "    # Combine 'year', 'hour', 'day', and 'month' columns into 'cet_timestamp'\n",
        "    df['cet_timestamp'] = df.apply(lambda row: datetime(row['year'], row['month'], row['day'], row['hour']), axis=1)\n",
        "\n",
        "\n",
        "    # Add country prefix to each column except 'cet_timestamp'\n",
        "    df.columns = [f'{country_prefix}_{col}' if col not in ['hour', 'day', 'month', 'year','cet_timestamp'] else col for col in df.columns]\n",
        "\n",
        "    return df\n",
        "\n",
        "# Iterate over the CSV files\n",
        "file_names = [\n",
        "    'AT_preprocessed_data.csv',\n",
        "    'BE_preprocessed_data.csv',\n",
        "    'BG_preprocessed_data.csv',\n",
        "    'CH_preprocessed_data.csv',\n",
        "    'CZ_preprocessed_data.csv',\n",
        "    'DE_preprocessed_data.csv',\n",
        "    'DK_preprocessed_data.csv',\n",
        "    'EE_preprocessed_data.csv',\n",
        "    'ES_preprocessed_data.csv',\n",
        "    'FI_preprocessed_data.csv',\n",
        "    'FR_preprocessed_data.csv',\n",
        "    'GB_preprocessed_data.csv',\n",
        "    'GR_preprocessed_data.csv',\n",
        "    'HR_preprocessed_data.csv',\n",
        "    'HU_preprocessed_data.csv',\n",
        "    'IE_preprocessed_data.csv',\n",
        "    'IT_preprocessed_data.csv',\n",
        "    'LT_preprocessed_data.csv',\n",
        "    'LU_preprocessed_data.csv',\n",
        "    'LV_preprocessed_data.csv',\n",
        "    'NL_preprocessed_data.csv',\n",
        "    'NO_preprocessed_data.csv',\n",
        "    'PL_preprocessed_data.csv',\n",
        "    'PT_preprocessed_data.csv',\n",
        "    'RO_preprocessed_data.csv',\n",
        "    'SE_preprocessed_data.csv',\n",
        "    'SI_preprocessed_data.csv',\n",
        "    'SK_preprocessed_data.csv'\n",
        "]\n",
        "\n",
        "csv_files = ['gs://final_preprocessed_files_when2heat/'+file for file in file_names]  # Update with your CSV file names and path\n",
        "\n",
        "for csv_file in csv_files:\n",
        "    # Read the CSV file\n",
        "    df = pd.read_csv(csv_file)\n",
        "\n",
        "    # Extract the country prefix from the file name\n",
        "    country_prefix = csv_file.split('/')[-1][:2]\n",
        "\n",
        "    # Modify the column names with the country prefix\n",
        "    df = fix_the_csv(df, country_prefix)\n",
        "\n",
        "    # Create a schema dictionary\n",
        "    dtypes_dic = df.dtypes.apply(lambda x: str(x)).to_dict()\n",
        "    ready_dtypes_dic = {k: change_types(dtypes_dic[k]) for k in dtypes_dic}\n",
        "    schema = [bigquery.SchemaField(k, ready_dtypes_dic[k]) for k in ready_dtypes_dic]\n",
        "\n",
        "    # Extract the table name from the CSV file name\n",
        "    table_id = csv_file.split('/')[-1]\n",
        "    table_id = table_id.replace(\".csv\", \"\")\n",
        "\n",
        "    # Create a new table in BigQuery\n",
        "    create_table(dataset_id, table_id, schema)\n",
        "\n",
        "    # Write the CSV into the newly created table\n",
        "    df.to_csv(table_id, index=False)  # Save the CSV file with the modified column names\n",
        "\n",
        "    # Load the CSV into the newly created table\n",
        "    load_csv_to_table(dataset_id, table_id, table_id)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EBLB2PHvCWb2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}