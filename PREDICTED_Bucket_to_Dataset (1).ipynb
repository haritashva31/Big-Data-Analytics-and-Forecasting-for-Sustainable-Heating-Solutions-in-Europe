{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Upload multiple CSV files from a Google Cloud Bucket to a Big Query dataset"
      ],
      "metadata": {
        "id": "EJcR73F8hKdr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Package installation"
      ],
      "metadata": {
        "id": "xZx1hcbzhmTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gcsfs"
      ],
      "metadata": {
        "id": "Pi64RhcWeLzO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ce68100-60e4-4cd2-901e-73145ab2a6ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gcsfs in /usr/local/lib/python3.10/dist-packages (2023.6.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from gcsfs) (3.9.3)\n",
            "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.10/dist-packages (from gcsfs) (4.4.2)\n",
            "Requirement already satisfied: fsspec==2023.6.0 in /usr/local/lib/python3.10/dist-packages (from gcsfs) (2023.6.0)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.10/dist-packages (from gcsfs) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.10/dist-packages (from gcsfs) (1.2.0)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.10/dist-packages (from gcsfs) (2.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gcsfs) (2.31.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (4.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.2->gcsfs) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.2->gcsfs) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.2->gcsfs) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib->gcsfs) (1.3.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->gcsfs) (2.11.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->gcsfs) (2.3.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->gcsfs) (2.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->gcsfs) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->gcsfs) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gcsfs) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->gcsfs) (2024.2.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs) (1.62.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs) (3.20.3)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage->gcsfs) (1.5.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "from google.cloud import bigquery\n",
        "import pandas as pd\n",
        "\n",
        "# Authenticate with Google Cloud\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Set up BigQuery client\n",
        "client = bigquery.Client(project='ty-mini-project-41')\n",
        "\n",
        "# Function to create a new table in BigQuery\n",
        "def create_table(dataset_id, table_id, schema):\n",
        "    dataset_ref = client.dataset(dataset_id)\n",
        "    table_ref = dataset_ref.table(table_id)\n",
        "\n",
        "\n",
        "    table = bigquery.Table(table_ref, schema=schema)\n",
        "    table = client.create_table(table)\n",
        "    print(f'Table {table.table_id} created successfully.')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R_uW1GJ2iNjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to read a csv file from a Google cloud Bucket and push its content to a table in Big Query\n",
        "def load_csv_to_table(dataset_id, table_id, csv_path):\n",
        "    dataset_ref = client.dataset(dataset_id)\n",
        "    table_ref = dataset_ref.table(table_id)\n",
        "\n",
        "    job_config = bigquery.LoadJobConfig()\n",
        "    job_config.skip_leading_rows = 1  # Skip the header row\n",
        "\n",
        "    with open(csv_path, 'rb') as source_file:\n",
        "        job = client.load_table_from_file(source_file, table_ref, job_config=job_config)\n",
        "\n",
        "    job.result()  # Wait for the job to complete\n",
        "\n",
        "    print(f'CSV file {csv_path} loaded into table {table_id} successfully.')"
      ],
      "metadata": {
        "id": "xEx64Vy95gH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following functions were included to standarize date formats within the files"
      ],
      "metadata": {
        "id": "IjK0mQhHkZUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "#find object columns\n",
        "\n",
        "def change_time(x):\n",
        "    format = '%m/%d/%Y %I:%M:%S %p'\n",
        "    my_date = datetime.strptime(x, format)\n",
        "    return my_date\n",
        "\n",
        "def fix_the_csv(df):\n",
        "  all_strings=list(df.dtypes[df.dtypes=='object'].index)\n",
        "  for var in all_strings:\n",
        "    try:\n",
        "      if df[var].apply(lambda x:1 if x.count(\" PM\")+x.count(' AM')>0 else 0).sum()>0:\n",
        "        df[var]=df[var].apply(lambda x: change_time(x))\n",
        "    except:\n",
        "      pass\n",
        "  return df\n",
        "\n",
        "def change_types(x):\n",
        "  if x.count('int')>0:\n",
        "    return \"NUMERIC\"\n",
        "  elif x.count('date')>0:\n",
        "    return 'DATETIME'\n",
        "  elif x.count('float')>0:\n",
        "    return 'BIGNUMERIC'\n",
        "  else:\n",
        "    return 'STRING'"
      ],
      "metadata": {
        "id": "6DjfmHb_-K9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The process need to receive the name of each file that you would like to add, so i included this function to list all files in a google cloud bucket"
      ],
      "metadata": {
        "id": "ZrYET-pW0sck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "\n",
        "def list_files_in_bucket(bucket_name):\n",
        "    # Initialize a client\n",
        "    client = storage.Client()\n",
        "\n",
        "    for blob in client.list_blobs(bucket_name):\n",
        "      print(str(blob))\n",
        "\n",
        "\n",
        "list_files_in_bucket('final_predicted_files_when2heat')\n"
      ],
      "metadata": {
        "id": "ng2L0jjprp0t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d1a2204-14ea-4d24-9fa5-b1f6833a3174"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Blob: final_predicted_files_when2heat, AT_predicted_df_predicted_df.csv, 1714047960010594>\n",
            "<Blob: final_predicted_files_when2heat, BE_predicted_df_predicted_df.csv, 1714048031119581>\n",
            "<Blob: final_predicted_files_when2heat, BG_predicted_df_predicted_df.csv, 1714047959998075>\n",
            "<Blob: final_predicted_files_when2heat, CH_predicted_df_predicted_df.csv, 1714047486808800>\n",
            "<Blob: final_predicted_files_when2heat, CZ_predicted_df_predicted_df.csv, 1714048063168641>\n",
            "<Blob: final_predicted_files_when2heat, DE_predicted_df_predicted_df.csv, 1714047692659772>\n",
            "<Blob: final_predicted_files_when2heat, DK_predicted_df_predicted_df.csv, 1714048031243220>\n",
            "<Blob: final_predicted_files_when2heat, EE_predicted_df_predicted_df.csv, 1714048074848408>\n",
            "<Blob: final_predicted_files_when2heat, ES_predicted_df_predicted_df.csv, 1714047466165570>\n",
            "<Blob: final_predicted_files_when2heat, FI_predicted_df_predicted_df.csv, 1714047722653462>\n",
            "<Blob: final_predicted_files_when2heat, FR_predicted_df_predicted_df.csv, 1714047845393800>\n",
            "<Blob: final_predicted_files_when2heat, GB_predicted_df_predicted_df.csv, 1714047595107878>\n",
            "<Blob: final_predicted_files_when2heat, GR_predicted_df_predicted_df.csv, 1714047884547221>\n",
            "<Blob: final_predicted_files_when2heat, HR_predicted_df_predicted_df.csv, 1714047773628457>\n",
            "<Blob: final_predicted_files_when2heat, HU_predicted_df_predicted_df.csv, 1714047681433490>\n",
            "<Blob: final_predicted_files_when2heat, IE_predicted_df_predicted_df.csv, 1714047767999115>\n",
            "<Blob: final_predicted_files_when2heat, IT_predicted_df_predicted_df.csv, 1714047791963728>\n",
            "<Blob: final_predicted_files_when2heat, LT_predicted_df_predicted_df.csv, 1714047931140551>\n",
            "<Blob: final_predicted_files_when2heat, LU_predicted_df_predicted_df.csv, 1714047590124711>\n",
            "<Blob: final_predicted_files_when2heat, LV_predicted_df_predicted_df.csv, 1714048000978879>\n",
            "<Blob: final_predicted_files_when2heat, NL_predicted_df_predicted_df.csv, 1714047467779118>\n",
            "<Blob: final_predicted_files_when2heat, NO_predicted_df_predicted_df.csv, 1714047791771791>\n",
            "<Blob: final_predicted_files_when2heat, PL_predicted_df_predicted_df.csv, 1714047884735441>\n",
            "<Blob: final_predicted_files_when2heat, PT_predicted_df_predicted_df.csv, 1714047526147339>\n",
            "<Blob: final_predicted_files_when2heat, RO_predicted_df_predicted_df.csv, 1714047532427040>\n",
            "<Blob: final_predicted_files_when2heat, SE_predicted_df_predicted_df.csv, 1714047467839588>\n",
            "<Blob: final_predicted_files_when2heat, SI_predicted_df_predicted_df.csv, 1714047554315058>\n",
            "<Blob: final_predicted_files_when2heat, SK_predicted_df_predicted_df.csv, 1714047618662564>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "from google.cloud import bigquery\n",
        "import pandas as pd\n",
        "\n",
        "# Authenticate with Google Cloud\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Set up BigQuery client\n",
        "client = bigquery.Client(project='ty-mini-project-41')\n",
        "\n",
        "# Define the dataset_id\n",
        "dataset_id = 'final_predicted_when2heat'  # Replace with your actual dataset ID\n",
        "\n",
        "# Function to create a new table in BigQuery\n",
        "def create_table(dataset_id, table_id, schema):\n",
        "    dataset_ref = client.dataset(dataset_id)\n",
        "    table_ref = dataset_ref.table(table_id)\n",
        "\n",
        "    table = bigquery.Table(table_ref, schema=schema)\n",
        "    table = client.create_table(table)\n",
        "    print(f'Table {table.table_id} created successfully.')\n",
        "\n",
        "# Function to read a csv file from a Google cloud Bucket and push its content to a table in Big Query\n",
        "def load_csv_to_table(dataset_id, table_id, csv_path):\n",
        "    dataset_ref = client.dataset(dataset_id)\n",
        "    table_ref = dataset_ref.table(table_id)\n",
        "\n",
        "    job_config = bigquery.LoadJobConfig()\n",
        "    job_config.skip_leading_rows = 1  # Skip the header row\n",
        "\n",
        "    with open(csv_path, 'rb') as source_file:\n",
        "        job = client.load_table_from_file(source_file, table_ref, job_config=job_config)\n",
        "\n",
        "    job.result()  # Wait for the job to complete\n",
        "\n",
        "    print(f'CSV file {csv_path} loaded into table {table_id} successfully.')\n",
        "\n",
        "# Function to fix the CSV DataFrame\n",
        "\n",
        "def fix_the_csv(df, country_prefix):\n",
        "    # Convert 'year', 'hour', 'day', and 'month' columns to integers\n",
        "    df['year'] = df['year'].astype(int)\n",
        "    df['month'] = df['month'].astype(int)\n",
        "    df['day'] = df['day'].astype(int)\n",
        "    df['hour'] = df['hour'].astype(int)\n",
        "\n",
        "    # Combine 'year', 'hour', 'day', and 'month' columns into 'cet_timestamp'\n",
        "    df['cet_timestamp'] = df.apply(lambda row: datetime(int(row['year']), int(row['month']), int(row['day']), int(row['hour'])), axis=1)\n",
        "\n",
        "    # Drop individual date and time columns\n",
        "    df = df.drop(columns=['year', 'hour', 'day', 'month'])\n",
        "\n",
        "    # Add country prefix to each column except 'cet_timestamp'\n",
        "    df.columns = [f'{country_prefix}_{col}' if col != 'cet_timestamp' else col for col in df.columns]\n",
        "\n",
        "    return df\n",
        "\n",
        "# Iterate over the CSV files\n",
        "file_names = [\n",
        "    'AT_predicted_df_predicted_df.csv',\n",
        "    'BE_predicted_df_predicted_df.csv',\n",
        "    'BG_predicted_df_predicted_df.csv',\n",
        "    'CH_predicted_df_predicted_df.csv',\n",
        "    'CZ_predicted_df_predicted_df.csv',\n",
        "    'DE_predicted_df_predicted_df.csv',\n",
        "    'DK_predicted_df_predicted_df.csv',\n",
        "    'EE_predicted_df_predicted_df.csv',\n",
        "    'ES_predicted_df_predicted_df.csv',\n",
        "    'FI_predicted_df_predicted_df.csv',\n",
        "    'FR_predicted_df_predicted_df.csv',\n",
        "    'GB_predicted_df_predicted_df.csv',\n",
        "    'GR_predicted_df_predicted_df.csv',\n",
        "    'HR_predicted_df_predicted_df.csv',\n",
        "    'HU_predicted_df_predicted_df.csv',\n",
        "    'IE_predicted_df_predicted_df.csv',\n",
        "    'IT_predicted_df_predicted_df.csv',\n",
        "    'LT_predicted_df_predicted_df.csv',\n",
        "    'LU_predicted_df_predicted_df.csv',\n",
        "    'LV_predicted_df_predicted_df.csv',\n",
        "    'NL_predicted_df_predicted_df.csv',\n",
        "    'NO_predicted_df_predicted_df.csv',\n",
        "    'PL_predicted_df_predicted_df.csv',\n",
        "    'PT_predicted_df_predicted_df.csv',\n",
        "    'RO_predicted_df_predicted_df.csv',\n",
        "    'SE_predicted_df_predicted_df.csv',\n",
        "    'SI_predicted_df_predicted_df.csv',\n",
        "    'SK_predicted_df_predicted_df.csv'\n",
        "]\n",
        "\n",
        "csv_files = ['gs://final_predicted_files_when2heat/'+file for file in file_names]  # Update with your CSV file names and path\n",
        "\n",
        "for csv_file in csv_files:\n",
        "    # Read the CSV file\n",
        "    df = pd.read_csv(csv_file)\n",
        "\n",
        "    # Extract the country prefix from the file name\n",
        "    country_prefix = csv_file.split('/')[-1][:2]\n",
        "\n",
        "    # Modify the column names with the country prefix\n",
        "    df = fix_the_csv(df, country_prefix)\n",
        "\n",
        "    # Create a schema dictionary\n",
        "    dtypes_dic = df.dtypes.apply(lambda x: str(x)).to_dict()\n",
        "    ready_dtypes_dic = {k: change_types(dtypes_dic[k]) for k in dtypes_dic}\n",
        "    schema = [bigquery.SchemaField(k, ready_dtypes_dic[k]) for k in ready_dtypes_dic]\n",
        "\n",
        "    # Extract the table name from the CSV file name\n",
        "    table_id = csv_file.split('/')[-1]\n",
        "    table_id = table_id.replace(\".csv\", \"\")\n",
        "\n",
        "    # Create a new table in BigQuery\n",
        "    create_table(dataset_id, table_id, schema)\n",
        "\n",
        "    # Write the CSV into the newly created table\n",
        "    df.to_csv(table_id, index=False)  # Save the CSV file with the modified column names\n",
        "\n",
        "    # Load the CSV into the newly created table\n",
        "    load_csv_to_table(dataset_id, table_id, table_id)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_h0-LwmDuIf",
        "outputId": "5dd8a775-4874-4e3f-b694-82d59b2d3268"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Table AT_predicted_df_predicted_df created successfully.\n",
            "CSV file AT_predicted_df_predicted_df loaded into table AT_predicted_df_predicted_df successfully.\n",
            "Table BE_predicted_df_predicted_df created successfully.\n",
            "CSV file BE_predicted_df_predicted_df loaded into table BE_predicted_df_predicted_df successfully.\n",
            "Table BG_predicted_df_predicted_df created successfully.\n",
            "CSV file BG_predicted_df_predicted_df loaded into table BG_predicted_df_predicted_df successfully.\n",
            "Table CH_predicted_df_predicted_df created successfully.\n",
            "CSV file CH_predicted_df_predicted_df loaded into table CH_predicted_df_predicted_df successfully.\n",
            "Table CZ_predicted_df_predicted_df created successfully.\n",
            "CSV file CZ_predicted_df_predicted_df loaded into table CZ_predicted_df_predicted_df successfully.\n",
            "Table DE_predicted_df_predicted_df created successfully.\n",
            "CSV file DE_predicted_df_predicted_df loaded into table DE_predicted_df_predicted_df successfully.\n",
            "Table DK_predicted_df_predicted_df created successfully.\n",
            "CSV file DK_predicted_df_predicted_df loaded into table DK_predicted_df_predicted_df successfully.\n",
            "Table EE_predicted_df_predicted_df created successfully.\n",
            "CSV file EE_predicted_df_predicted_df loaded into table EE_predicted_df_predicted_df successfully.\n",
            "Table ES_predicted_df_predicted_df created successfully.\n",
            "CSV file ES_predicted_df_predicted_df loaded into table ES_predicted_df_predicted_df successfully.\n",
            "Table FI_predicted_df_predicted_df created successfully.\n",
            "CSV file FI_predicted_df_predicted_df loaded into table FI_predicted_df_predicted_df successfully.\n",
            "Table FR_predicted_df_predicted_df created successfully.\n",
            "CSV file FR_predicted_df_predicted_df loaded into table FR_predicted_df_predicted_df successfully.\n",
            "Table GB_predicted_df_predicted_df created successfully.\n",
            "CSV file GB_predicted_df_predicted_df loaded into table GB_predicted_df_predicted_df successfully.\n",
            "Table GR_predicted_df_predicted_df created successfully.\n",
            "CSV file GR_predicted_df_predicted_df loaded into table GR_predicted_df_predicted_df successfully.\n",
            "Table HR_predicted_df_predicted_df created successfully.\n",
            "CSV file HR_predicted_df_predicted_df loaded into table HR_predicted_df_predicted_df successfully.\n",
            "Table HU_predicted_df_predicted_df created successfully.\n",
            "CSV file HU_predicted_df_predicted_df loaded into table HU_predicted_df_predicted_df successfully.\n",
            "Table IE_predicted_df_predicted_df created successfully.\n",
            "CSV file IE_predicted_df_predicted_df loaded into table IE_predicted_df_predicted_df successfully.\n",
            "Table IT_predicted_df_predicted_df created successfully.\n",
            "CSV file IT_predicted_df_predicted_df loaded into table IT_predicted_df_predicted_df successfully.\n",
            "Table LT_predicted_df_predicted_df created successfully.\n",
            "CSV file LT_predicted_df_predicted_df loaded into table LT_predicted_df_predicted_df successfully.\n",
            "Table LU_predicted_df_predicted_df created successfully.\n",
            "CSV file LU_predicted_df_predicted_df loaded into table LU_predicted_df_predicted_df successfully.\n",
            "Table LV_predicted_df_predicted_df created successfully.\n",
            "CSV file LV_predicted_df_predicted_df loaded into table LV_predicted_df_predicted_df successfully.\n",
            "Table NL_predicted_df_predicted_df created successfully.\n",
            "CSV file NL_predicted_df_predicted_df loaded into table NL_predicted_df_predicted_df successfully.\n",
            "Table NO_predicted_df_predicted_df created successfully.\n",
            "CSV file NO_predicted_df_predicted_df loaded into table NO_predicted_df_predicted_df successfully.\n",
            "Table PL_predicted_df_predicted_df created successfully.\n",
            "CSV file PL_predicted_df_predicted_df loaded into table PL_predicted_df_predicted_df successfully.\n",
            "Table PT_predicted_df_predicted_df created successfully.\n",
            "CSV file PT_predicted_df_predicted_df loaded into table PT_predicted_df_predicted_df successfully.\n",
            "Table RO_predicted_df_predicted_df created successfully.\n",
            "CSV file RO_predicted_df_predicted_df loaded into table RO_predicted_df_predicted_df successfully.\n",
            "Table SE_predicted_df_predicted_df created successfully.\n",
            "CSV file SE_predicted_df_predicted_df loaded into table SE_predicted_df_predicted_df successfully.\n",
            "Table SI_predicted_df_predicted_df created successfully.\n",
            "CSV file SI_predicted_df_predicted_df loaded into table SI_predicted_df_predicted_df successfully.\n",
            "Table SK_predicted_df_predicted_df created successfully.\n",
            "CSV file SK_predicted_df_predicted_df loaded into table SK_predicted_df_predicted_df successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DGLngjE5D50_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}